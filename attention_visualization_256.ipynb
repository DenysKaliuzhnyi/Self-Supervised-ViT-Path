{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Visualization tools are adapted from https://github.com/facebookresearch/dino.\n",
    "'''\n",
    "\n",
    "# Base Dependencies\n",
    "import argparse\n",
    "import colorsys\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# LinAlg / Stats / Plotting Dependencies\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "import skimage.io\n",
    "from skimage.measure import find_contours\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Torch Dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms as pth_transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Utils\n",
    "import nn_encoder_arch.vision_transformer as vits\n",
    "from attention_visualization_utils import create_256x256_map_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pretrained ViT-S/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take key teacher in provided checkpoint dict\n",
      "Pretrained weights found at ./ckpts/vits_tcga_brca_dino.pt and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v'])\n"
     ]
    }
   ],
   "source": [
    "arch = 'vit_small'\n",
    "patch_size = 16\n",
    "pretrained_weights = './ckpts/vits_tcga_brca_dino.pt'\n",
    "checkpoint_key = 'teacher'\n",
    "threshold = 0.5\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "### Build model\n",
    "model = vits.__dict__[arch](patch_size=patch_size, num_classes=0)\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "if os.path.isfile(pretrained_weights):\n",
    "    state_dict = torch.load(pretrained_weights, map_location=\"cpu\")\n",
    "    if checkpoint_key is not None and checkpoint_key in state_dict:\n",
    "        print(f\"Take key {checkpoint_key} in provided checkpoint dict\")\n",
    "        state_dict = state_dict[checkpoint_key]\n",
    "    # remove `module.` prefix\n",
    "    state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "    # remove `backbone.` prefix induced by multicrop wrapper\n",
    "    state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n",
    "    msg = model.load_state_dict(state_dict, strict=False)\n",
    "    print('Pretrained weights found at {} and loaded with msg: {}'.format(pretrained_weights, msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = list(range(6))\n",
    "IMG_SIZE_DINO = 256  # image size of dino model (do not change)\n",
    "IMG_SIZE_MINE = 1024  # image size of your images (change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "images = glob.glob('/gpfs/space/projects/PerkinElmer/testis/tiles/train2017/*.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(Image.open('attention_visualization_results/image_4k/image_0_0.png_mask_th.png')).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(Image.open('attention_visualization_results/image_4k/image_0_0.png_mask_th0.5_head0.png')).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(Image.open('attention_visualization_results/oneshot/18H14294I_lvl0_'\n",
    "#                     '103424_208896/18H14294I_lvl0_103424_208896.png_mask_th0.5_head0.png')).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(Image.open('attention_visualization_results/oneshot/18H14294I_lvl0_'\n",
    "#                     '103424_208896/18H14294I_lvl0_103424_208896.png_mask_th0.5_head0_annotated.png')).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(Image.open('attention_visualization_results/oneshot/18H14294I_lvl0_'\n",
    "#                     '103424_208896/18H14294I_lvl0_103424_208896.png_mask_th.png')).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(Image.open('attention_visualization_results/'\n",
    "#            'joined_patches/18H14294II_lvl0_70656_167936/18H14294II_lvl0_70656_167936_head0.png')).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(Image.open('attention_visualization_results/'\n",
    "#            'joined_patches/18H14294II_lvl0_70656_167936/18H14294II_lvl0_70656_167936_head0_annotated.png')).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(Image.open('attention_visualization_results/'\n",
    "#            'per_patch/18H14294I_lvl0_103424_208896/image_0_0.png_mask_th0.5_head0.png')).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(Image.open('attention_visualization_results/'\n",
    "#            'per_patch/18H14294I_lvl0_103424_208896/image_256_0.png_mask_th.png')).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 257/257 [1:15:15<00:00, 17.57s/it]\n"
     ]
    }
   ],
   "source": [
    "# run with the original image size\n",
    "# save to attention_visualization_results/oneshot\n",
    "\n",
    "for img_path in tqdm(images):\n",
    "    img = Image.open(img_path)\n",
    "    img_fname = os.path.basename(img_path)\n",
    "    output_dir = os.path.join('attention_visualization_results', 'oneshot', img_fname[:-4])\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    create_256x256_map_concat(model, img, img_fname, output_dir, \n",
    "                              image_size=(IMG_SIZE_MINE, IMG_SIZE_MINE), display=False, \n",
    "                              which_concat=heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [29:32<00:00, 30.55s/it]\n"
     ]
    }
   ],
   "source": [
    "# run with patches\n",
    "# save to attention_visualization_results/per_patch\n",
    "\n",
    "for img_path in tqdm(images):\n",
    "    img = np.array(Image.open(img_path))\n",
    "\n",
    "    img_fname = os.path.basename(img_path)\n",
    "    output_dir = os.path.join('attention_visualization_results', 'per_patch', img_fname[:-4])\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    assert IMG_SIZE_MINE / IMG_SIZE_DINO == int(IMG_SIZE_MINE / IMG_SIZE_DINO)\n",
    "    \n",
    "    for i in range(0, IMG_SIZE_MINE, IMG_SIZE_DINO):\n",
    "        for j in range(0, IMG_SIZE_MINE, IMG_SIZE_DINO):\n",
    "            patch = Image.fromarray(img[i:(i+IMG_SIZE_DINO),j:(j+IMG_SIZE_DINO),:])\n",
    "            subimg_fname = 'image_%d_%d.png' % (i,j)\n",
    "            create_256x256_map_concat(model, patch, subimg_fname, output_dir,\n",
    "                                      image_size=(IMG_SIZE_DINO,IMG_SIZE_DINO), display=False, \n",
    "                                      which_concat=heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 257/257 [38:22<00:00,  8.96s/it]\n"
     ]
    }
   ],
   "source": [
    "# join patches into one image\n",
    "# save to attention_visualization_results/joined_patches\n",
    "\n",
    "\n",
    "def join_patches(files):\n",
    "    patches = []\n",
    "    img_full = np.empty((IMG_SIZE_MINE, IMG_SIZE_MINE, 4), dtype='int')\n",
    "\n",
    "    for img_path in files:\n",
    "        img = np.array(Image.open(img_path))\n",
    "        assert img.shape == (IMG_SIZE_DINO, IMG_SIZE_DINO, 4), str(img.shape)\n",
    "        \n",
    "        img_name = os.path.basename(img_path)\n",
    "        _, i, j = img_name.split('.')[0].split('_')\n",
    "\n",
    "        i, j = int(i), int(j)\n",
    "        img_full[i:i+IMG_SIZE_DINO, j:j+IMG_SIZE_DINO] = img\n",
    "    \n",
    "    return img_full\n",
    "\n",
    "\n",
    "for img_path in tqdm(images):\n",
    "    img_fname = os.path.basename(img_path)\n",
    "    \n",
    "    for head in heads:\n",
    "        patches_dir = os.path.join('attention_visualization_results', 'per_patch', img_fname[:-4])    \n",
    "        patches_pat = os.path.join(patches_dir, f'*th0.5_head{head}.png')\n",
    "        patch_files = glob.glob(patches_pat)\n",
    "\n",
    "        full_img = join_patches(patch_files)\n",
    "\n",
    "        full_img_path = os.path.join('attention_visualization_results', 'joined_patches', img_fname[:-4])\n",
    "        os.makedirs(full_img_path, exist_ok=True)\n",
    "\n",
    "        pil_img = Image.fromarray(np.uint8(full_img))\n",
    "        pil_img.save(os.path.join(full_img_path, img_fname[:-4]+f'_head{head}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_id(file_name):\n",
    "    for img in coco['images']:\n",
    "        if img['file_name'] == file_name:\n",
    "            return img['id']\n",
    "        \n",
    "def get_img_annotations(img_id):\n",
    "    anns = []\n",
    "    for ann in coco['annotations']:\n",
    "        if ann['image_id'] == img_id:\n",
    "            anns.append(ann)\n",
    "    return anns\n",
    "\n",
    "import json\n",
    "\n",
    "ANNOTATION_FILE = '/gpfs/space/projects/PerkinElmer/testis/tiles/annotations/instances_train2017.json'\n",
    "with open(ANNOTATION_FILE, 'r') as f:\n",
    "    coco = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 257/257 [23:25<00:00,  5.47s/it]\n"
     ]
    }
   ],
   "source": [
    "# draw bboxes over produced images (original image size, at attention_visualization_results/oneshot)\n",
    "\n",
    "from PIL import ImageDraw\n",
    "\n",
    "for dir_path in tqdm(glob.glob('attention_visualization_results/oneshot/*')):\n",
    "    dir_name = os.path.basename(dir_path)\n",
    "    \n",
    "    for head in heads:\n",
    "        img_path = os.path.join(dir_path, dir_name+f'.png_mask_th0.5_head{head}.png')\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        assert np.array(img).shape == (IMG_SIZE_MINE, IMG_SIZE_MINE, 4), str(np.array(img).shape)\n",
    "\n",
    "        for ann in get_img_annotations(get_image_id(dir_name+'.png')):\n",
    "            x, y, w, h = ann['bbox']\n",
    "\n",
    "            img_draw = ImageDraw.Draw(img)\n",
    "            img_draw.rectangle((x, y, x+w, y+h), width=2, outline='green')\n",
    "\n",
    "        img.save(os.path.join('attention_visualization_results', 'oneshot', \n",
    "                              dir_name, dir_name+f'.png_mask_th0.5_head{head}_annotated.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 257/257 [28:14<00:00,  6.59s/it]\n"
     ]
    }
   ],
   "source": [
    "# draw bboxes over produced images (joined patches, at attention_visualization_results/joined_patches)\n",
    "\n",
    "from PIL import ImageDraw\n",
    "\n",
    "for dir_path in tqdm(glob.glob('attention_visualization_results/joined_patches/*')):\n",
    "    dir_name = os.path.basename(dir_path)\n",
    "    \n",
    "    for head in heads:\n",
    "        img_path = os.path.join(dir_path, dir_name+f'_head{head}.png')\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        for ann in get_img_annotations(get_image_id(dir_name+'.png')):\n",
    "            x, y, w, h = ann['bbox']\n",
    "\n",
    "            img_draw = ImageDraw.Draw(img)\n",
    "            img_draw.rectangle((x, y, x+w, y+h), width=8, outline='green')\n",
    "\n",
    "        img.save(os.path.join('attention_visualization_results', 'joined_patches', \n",
    "                              dir_name, dir_name+f'_head{head}_annotated.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15G\tattention_visualization_results/\n"
     ]
    }
   ],
   "source": [
    "!du -sh attention_visualization_results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls attention_visualization_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtproject",
   "language": "python",
   "name": "mtproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
